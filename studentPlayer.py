#!/usr/bin/env python
__author__ = 'Cristiano Vagos, Jo√£o Pedro Fonseca'
__email__ = 'cristianovagos@ua.pt, jpedrofonseca@ua.pt'
__license__ = "GPL"
__version__ = "1.0"
__maintainer__ = "Cristiano Vagos"

"""
Introdu√ß√£o √† Intelig√™ncia Artificial @ DETI-UA 2016/17
Trabalho Pr√°tico de Grupo: Blackjack

Cristiano Vagos (65169), Jo√£o Pedro Fonseca (73779)

------------------------------------------------------------------------------------------------

Agente que joga sozinho Blackjack, com o c√≥digo fornecido pelos professores.
Opt√°mos por escolher a estrat√©gia de AI Reinforcement Learning, que faz com
que o agente aprenda sozinho a tomar decis√µes conforme os jogos que vai fazendo
e os resultados obtidos dos mesmos.
(Retirado do livro "Reinforcement Learning: An Introduction")
(link: https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf)

Algumas ideias aqui implementadas s√£o baseadas em c√≥digo j√° existente, onde s√£o feitos
dois m√©todos de Reinforcement Learning (Monte Carlo ES e Q-Learning):
(c√≥digo: https://inst.eecs.berkeley.edu/~cs188/sp08/projects/blackjack/blackjack.py)

A nossa estrat√©gia passa por jogar de forma completamente aleat√≥ria at√© um n√∫mero limite
de jogadas para cada estado poss√≠vel. Ent√£o, os dados s√£o guardados em 2 dicion√°rios que
guardam todos os estados poss√≠veis e a probabilidade de acerto, que √© aumentada/diminuida
conforme a recompensa de cada jogo (1 em caso de vit√≥ria, 0 para empate, -1 para derrota).
Todas as ac√ß√µes s√£o escolhidas pelo agente, de forma completamente aut√≥noma tendo em base
os jogos que j√° efetuou at√© ao momento.

Depois do n√∫mero limite de jogadas para cada tipo de jogada (Hit, Stand ou Double-Down)
o agente joga a melhor op√ß√£o determinada pelo hist√≥rico de jogos que fez at√© ao momento.
A melhor ac√ß√£o prov√©m do dicion√°rio respetivo (primeira / pr√≥ximas jogadas), e √© escolhida
a ac√ß√£o que tem a maior m√©dia de recompensa (quanto mais pr√≥ximo de 1, melhor, e quanto mais
pr√≥ximo de -1 pior).
Por isso o r√°cio de vit√≥rias ir√° aumentar ao longo do tempo em que a tabela √© preenchida.

Guardamos os dados (Tabelas Q + Contagens de Ocorr√™ncias) num ficheiro que √© habitualmente
lido e gravado com os dados que vai obtendo no decorrer dos jogos. Conforme o estado do jogo atual
(m√£o do dealer, m√£o do jogador, se o jogador tem "hard" ou "soft hand"), o agente escolhe
ent√£o uma op√ß√£o de entre as dispon√≠veis (Hit / Stand / Double-Down / Surrender).

O sistema de apostas usado no nosso agente √© o "Up and Pull Betting System".
(Retirado de: http://www.countingedge.com/blackjack-money-management/)
Ap√≥s os testes confirma-se que √© uma boa estrat√©gia de apostas para um bom ganho e menor risco.

------------------------------------------------------------------------------------------------

Packages n√£o-nativos necess√°rios √† execu√ß√£o: numpy

"""

import os
import os.path

import card
import numpy as np
import pickle
import random
from player import Player
from collections import defaultdict

class StudentPlayer(Player):
    def __init__(self, name="Zeljko Ranogajec", money=0):
        super(StudentPlayer, self).__init__(name, money)
        self.name = name
        self.money = money

        """
        Op√ß√£o para visualizar no terminal tudo o que se passa
        no agente. Estados atuais, valores, e decis√µes.
        """
        self.verbose = False

        """
        Inicializa√ß√£o das estruturas de dados

        - states    : todos os estados poss√≠veis
        - qFirst    : tabela Q da primeira jogada (devido ao Double-Down)
        - q         : tabela Q das pr√≥ximas jogadas
        - countFirst: contagem da primeira jogada (devido ao Double-Down)
        - count     : contagem das restantes jogadas (s√≥ guarda Hit/Stand)
        """
        self.states = initStates()
        self.qFirst = initActions(self.states)
        self.q = initActions(self.states)
        self.countFirst = initCount(self.qFirst)
        self.count = initCount(self.q)

        """
        Inicializa√ß√£o das vari√°veis auxiliares

        - winStreak    : contagem do n¬∫ de vit√≥rias sucessivas
        - firstPlay    : verifica√ß√£o se √© a primeira joga
        - learnFactor  : n¬∫ de jogos necess√°rios para aprender (p/ cada jogada)da
        - surrenderFlag: "probabilidade" limite para fazer Surrender
        - results      : dicionario que guarda os estados decorridos no jogo (pr√≥ximas jogadas)
        - firstResults : dicionario que guarda os estados decorridos no jogo (primeira jogada)
        """
        self.winStreak = 0
        self.firstPlay = True
        self.learnFactor = 150
        self.surrenderFlag = -0.65
        self.results = defaultdict(int)
        self.firstResults = defaultdict(int)

        """
        Inicializa√ß√£o das vari√°veis de contagem

        Usadas para efeitos de logging, pois √© criado um ficheiro
        para ver os resultados atuais e totais dos jogos
        """
        self.games = 0
        self.currentGames = 0
        self.fileGames = 0
        self.wins = 0
        self.draws = 0
        self.losses = 0
        self.twins = 0
        self.tlosses = 0
        self.tdraws = 0

        """
        Verifica√ß√£o do ficheiro que guarda os dados dos jogos
        efetuados pelo agente
        """
        # Verificar se o ficheiro existe
        if not os.path.isfile("studentPlayer.data"):
            # O ficheiro n√£o existe
            print("Creating file... (press ENTER)")
            input()
            # Cria o ficheiro
            self.createFile()
        # Verificar se o ficheiro est√° vazio
        elif os.stat("studentPlayer.data").st_size == 0:
            print("File empty. Writing something... (press ENTER)")
            input()
            # Escreve o dicionario no ficheiro s√≥ para o ficheiro ter alguma coisa
            self.writeFile()
        else:
            print("File found. Reading... (press ENTER)")
            input()
            # Vamos ler o ficheiro e escrever o seu conteudo no dicionario
            self.readFile()

    def play(self, dealer, players):
        if self.verbose:
            self.debug_state(dealer, players)

        # Obter valores da minha m√£o
        playerHand, playerValue = self.getPlayerValue(players)

        # Se for primeira jogada vamos ler valor da m√£o do dealer
        if self.firstPlay:
            self.dealerValue = self.getDealerValue(dealer)

        playerAce = self.getUseableAce(playerHand)

        state = (self.dealerValue, playerValue, playerAce)

        if self.learning(state, self.firstPlay):
        #if self.learningEpsilon():
            action = self.getRandomAction(self.firstPlay)
        else:
            action = self.getBestAction(state, self.firstPlay)

        sa = (state, action)

        if self.firstPlay:
            self.firstPlay = False
            self.countFirst[sa] += 1
            self.firstResults[sa] = 0
        else:
            self.count[sa] += 1
            self.results[sa] = 0

        # Joga com a a√ß√£o escolhida
        return action

    # Retirado dos ficheiros fornecidos
    def debug_state(self, dealer, players):
        print("--------------------------------------------")
        print("{:10s}: {!s:32s} = {}".format("Dealer", ['üé¥ '] + dealer.hand, card.value(dealer.hand)))
        for p in players:
            print("{:10s}: {!s:32s} = {}".format(p.player.name, p.hand, card.value(p.hand)))

    def bet(self, dealer, players):
        return self.bets[self.winStreak]

    def want_to_play(self, rules):
        self.min_bet = rules.min_bet
        self.max_bet = rules.max_bet
        self.bets = list(range(1, self.max_bet + 1))
        self.bets[0] = 2
        return True

    def payback(self, prize):
        # Jogo j√° acabou, vamos recolher o pr√©mio
        super(StudentPlayer, self).payback(prize)
        self.money += prize

        # Saber se perdemos ou ganhamos
        draw = (prize == 0)
        win = (prize > 0)

        # Qual a nossa recompensa?
        # 1 - Ganhamos
        # 0 - Empatamos
        # -1 - Perdemos
        if win:
            if self.verbose:
                print("Ganh√°mos! :)")
            if self.winStreak+1 < len(self.bets):
                self.winStreak += 1

            reward = 1
            self.wins += 1
            self.twins += 1
        elif draw:
            if self.verbose:
                print("Empate :/")
            reward = 0
            self.draws += 1
            self.tdraws += 1
            self.winStreak = 0
        else:
            if self.verbose:
                print("Perdemos :(")
            reward = -1
            self.losses += 1
            self.tlosses += 1
            self.winStreak = 0

        # Atualizar os resultados do jogo de acordo com a nossa recompensa
        # (em caso de Double-Down a recompensa duplica pois foi feito o dobro da aposta)
        for key in self.firstResults:
            if key[1] == 'd':
                self.firstResults[key] = reward * 2
            else:
                self.firstResults[key] = reward

        for key in self.results:
            if key[1] == 'u':
                break
            self.results[key] = reward

        if len(self.firstResults) > 0:
            self.qFirst = updateTable(self.qFirst, self.countFirst, self.firstResults)

        if len(self.results) > 0:
            self.q = updateTable(self.q, self.count, self.results)

        # Prepara para o pr√≥ximo jogo
        self.results = defaultdict(int)
        self.firstResults = defaultdict(int)
        self.firstPlay = True
        self.games += 1
        self.currentGames += 1
        self.fileGames += 1

        # Escrevemos a tabela no ficheiro a cada 10000 jogos
        if self.games % 10000 == 0:
            self.writeFile()
            self.writeLogFile()

    """
    --------------------------------------------------------------------------------------------------------
    FUN√á√ïES AUXILIARES

    - getDealerValue  (obter o valor do dealer)
    - getPlayerValue  (obter o valor do jogador)
    - getUseableAce   (saber se o jogador tem soft/hard hand)
    - learning        (saber se o jogador est√° em aprendizagem)
    - getBestAction   (obter a melhor ac√ß√£o)
    - getRandomAction (obter uma ac√ß√£o aleat√≥ria dependendo se √© a primeira jogada ou n√£o)
    - writeLogFile    (escrever um ficheiro de log para acompanhar os resultados parciais e totais)
    - createFile      (cria√ß√£o do ficheiro com os dados do agente)
    - readFile        (leitura do ficheiro com os dados do agente)
    - writeFile       (escrita do ficheiro com os dados do agente)
    --------------------------------------------------------------------------------------------------------
    """

    # Obter valor do dealer
    def getDealerValue(self, dealer):
        dealerHand = card.value(dealer.hand)
        if len([c for c in dealer.hand if c.is_ace()]) > 0:
            dealerHand = 1
        return dealerHand

    # Obter valor da minha m√£o
    def getPlayerValue(self, players):
        for p in players:
            if p.player.name == self.name:
                return p.hand, card.value(p.hand)

    # Saber se tenho soft ou hard hand
    def getUseableAce(self, hand):
        numberAces = len([c for c in hand if c.is_ace()])
        if numberAces == 0:
            return False
        elif numberAces == 2 and len(hand) == 2 and (card.value(hand) == 12 or card.value(hand) == 2):
            return True
        handNoAce = card.value([c for c in hand if not c.is_ace()])
        return True if numberAces + handNoAce == card.value(hand) else False

    def learning(self, state, first):
        if first:
            stand = self.countFirst[(state, 's')]
            hit = self.countFirst[(state, 'h')]
            dd = self.countFirst[(state, 'd')]
            if (stand <= self.learnFactor or hit <= self.learnFactor or dd <= self.learnFactor):
                # True
                # -> epsilon greedy strategy: vamos deix√°-lo explorar outras op√ß√µes em 1% das vezes
                return False if random.random() < 0.01 else True
            return False
        stand = self.count[(state, 's')]
        hit = self.count[(state, 'h')]
        if (stand <= self.learnFactor or hit <= self.learnFactor):
            #True
            # -> epsilon greedy strategy: vamos deix√°-lo explorar outras op√ß√µes em 1% das vezes
            return False if random.random() < 0.01 else True
        return False

    def getBestAction(self, state, first):
        if first:
            hit = self.qFirst[(state, 'h')]
            stand = self.qFirst[(state, 's')]
            dd = self.qFirst[(state, 'd')]
            tmp = np.argmax(np.array([hit, stand, dd]))
            if tmp == 0:
                return 'h'
            elif tmp == 1:
                return 's'
            return 'd'
        hit = self.q[(state, 'h')]
        stand = self.q[(state, 's')]
        tmp = np.argmax(np.array([hit, stand]))
        aux = max(hit, stand)
        if aux <= self.surrenderFlag:
            return 'u'
        if tmp == 0:
            return 'h'
        return 's'

    def getRandomAction(self, first):
        if first:
            r = random.randint(0, 2)
            if r == 0:
                return 'h'
            elif r == 1:
                return 's'
            return 'd'
        return 'h' if random.random() < 0.5 else 's'

    def writeLogFile(self):
        # Escreve o dicionario no ficheiro
        file = open("studentPlayer-log.txt", "w")
        file.write("-------------------------------------------\n")
        file.write("Played {} games, won {}, draw {}, loss {}\n".format(self.currentGames, self.wins, self.draws, self.losses))
        file.write("Win Rate: {}%\n".format(self.wins/self.currentGames * 100))
        file.write("Total {} games, won {}, draw {}, loss {}\n".format(self.games, self.twins, self.tdraws, self.tlosses))
        file.write("Total Win Rate: {}%\n".format(self.twins / self.games * 100))
        self.losses = 0
        self.wins = 0
        self.draws = 0
        self.currentGames = 0
        file.close()

    def createFile(self):
        # Cria o ficheiro e fecha-o
        file = open("studentPlayer.data", "w+")
        file.close()

    def readFile(self):
        # Abrimos o dicionario em read-only mode e em binary mode
        file = open("studentPlayer.data", "rb")
        file.seek(0)
        tmpList = pickle.load(file)
        self.q = tmpList[0]
        self.qFirst = tmpList[1]
        self.count = tmpList[2]
        self.countFirst = tmpList[3]
        self.fileGames = tmpList[4]
        file.close()

    def writeFile(self):
        # Escreve o dicionario no ficheiro
        file = open("studentPlayer.data", "wb")
        file.seek(0)
        file.truncate()
        file.seek(0)
        tmpList = [self.q, self.qFirst, self.count, self.countFirst, self.fileGames]
        pickle.dump(tmpList, file)
        file.close()

"""
--------------------------------------------------------------------------------------------------------
FUN√á√ïES AUXILIARES (extra-classe)

- initStates    : devolve uma lista com os estados poss√≠veis no jogo
- initActions   : devolve um dicion√°rio com os estados/ac√ß√µes poss√≠veis no jogo
- initCount     : devolve um dicion√°rio para a contagem de ocorr√™ncias dos estados/ac√ß√µes
- updateTable   : atualiza a tabela Q dada com os dados da contagem e resultados obtidos num jogo
--------------------------------------------------------------------------------------------------------
"""

def initStates():
    states = []
    for dealer in range(1, 11):
        for player in range(4, 21):
            states.append((dealer, player, False))
            states.append((dealer, player, True))
    return states

def initActions(states):
    q = defaultdict(float)
    for state in states:
        q[(state, 's')] = 0.0
        q[(state, 'h')] = 0.0
        q[(state, 'd')] = 0.0
    return q

def initCount(actions):
    count = defaultdict(int)
    for action in actions:
        count[action] = 0
    return count

def updateTable(table, count, results):
    for key in results:
        table[key] = table[key] + (1 / count[key]) * (results[key] - table[key])
    return table
